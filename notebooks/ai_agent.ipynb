{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    # dimensions: Optional[int] = None, # Can specify dimensions with new text-embedding-3 models\n",
    "    # azure_endpoint=\"https://<your-endpoint>.openai.azure.com/\", If not provided, will read env variable AZURE_OPENAI_ENDPOINT\n",
    "    # api_key=... # Can provide an API key directly. If missing read env variable AZURE_OPENAI_API_KEY\n",
    "    # openai_api_version=..., # If not provided, will read env variable AZURE_OPENAI_API_VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "# Crear el objeto de embeddings\n",
    "# embedding_function = GPT4AllEmbeddings()\n",
    "\n",
    "\n",
    "embedding_function = AzureOpenAIEmbeddings(\n",
    "    model=deployment_name,  # Nombre del modelo desplegado\n",
    "    api_key=openai_api_key,\n",
    "    azure_endpoint=openai_api_base,\n",
    "    openai_api_version=openai_api_version,\n",
    ")\n",
    "\n",
    "# # Cargar la base de datos existente\n",
    "# persist_directory = \"./data/chroma_langchain_db\"\n",
    "# vectorstore = Chroma(collection_name=\"products\", \n",
    "#                      embedding_function=embedding_function, \n",
    "#                      persist_directory=persist_directory)\n",
    "\n",
    "\n",
    "\n",
    "# Datos del producto\n",
    "products = [\n",
    "    {\"id\": 1, \"name\": \"Collar de perlas\", \"color\": \"rosa\", \"price\": 50.0, \"description\": \"Un elegante collar de perlas.\"},\n",
    "    {\"id\": 2, \"name\": \"Anillo de diamantes\", \"color\": \"blanco\", \"price\": 200.0, \"description\": \"Anillo con diamantes.\"},\n",
    "    {\"id\": 3, \"name\": \"Collar de perlas\", \"color\": \"blanco\", \"price\": 50.0, \"description\": \"Un elegante collar de perlas.\"},\n",
    "    {\"id\": 4, \"name\": \"Anillo de diamantes\", \"color\": \"blanco\", \"price\": 200.0, \"description\": \"Anillo con diamantes.\"},\n",
    "    # Añadir más productos aquí\n",
    "]\n",
    "\n",
    "# Crear las cadenas concatenadas y metadatos\n",
    "texts = []\n",
    "metadatas = []\n",
    "for product in products:\n",
    "    text = f\"Producto: {product['name']}. Color: {product['color']}. Precio: {product['price']}. Descripción: {product['description']}.\"\n",
    "    metadata = {\"id\": product['id'], \"color\": product['color'], \"price\": product['price']}\n",
    "    texts.append(text)\n",
    "    metadatas.append(metadata)\n",
    "\n",
    "# Crear el índice FAISS con los textos y metadatos\n",
    "vectorstore = FAISS.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embedding_function,\n",
    "    metadatas=metadatas\n",
    ")\n",
    "\n",
    "# Guardar el índice FAISS en disco (opcional)\n",
    "faiss_index_file = \"./faiss_index\"\n",
    "vectorstore.save_local(faiss_index_file)\n",
    "\n",
    "# Cargar el índice FAISS desde disco (si lo guardaste previamente)\n",
    "loaded_vectorstore = FAISS.load_local(\n",
    "    folder_path=faiss_index_file,\n",
    "    embeddings=embedding_function,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "# Añadir los textos y metadatos a la base de datos\n",
    "# vectorstore.add_texts(texts, metadatas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'AzureOpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002297CB75BD0>, search_kwargs={})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consultar el índice\n",
    "query = \"Color blanco\"\n",
    "results = loaded_vectorstore.similarity_search(query, k=10)\n",
    "\n",
    "# Mostrar los resultados\n",
    "for result in results:\n",
    "    print(result.page_content, result.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Color blanco'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mloaded_vectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_by_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mColor blanco\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\JMgonzalez\\Documents\\pruebas\\retail_jewelry\\retail_web_jewelry\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:582\u001b[0m, in \u001b[0;36mFAISS.similarity_search_by_vector\u001b[1;34m(self, embedding, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search_by_vector\u001b[39m(\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    561\u001b[0m     embedding: List[\u001b[38;5;28mfloat\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    566\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m    567\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to embedding vector.\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \n\u001b[0;32m    569\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;124;03m        List of Documents most similar to the embedding.\u001b[39;00m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 582\u001b[0m     docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score_by_vector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfetch_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[1;32mc:\\Users\\JMgonzalez\\Documents\\pruebas\\retail_jewelry\\retail_web_jewelry\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:414\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score_by_vector\u001b[1;34m(self, embedding, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;124;03m    in float for each. Lower score represents more similarity.\u001b[39;00m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    413\u001b[0m faiss \u001b[38;5;241m=\u001b[39m dependable_faiss_import()\n\u001b[1;32m--> 414\u001b[0m vector \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([embedding], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_L2:\n\u001b[0;32m    416\u001b[0m     faiss\u001b[38;5;241m.\u001b[39mnormalize_L2(vector)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Color blanco'"
     ]
    }
   ],
   "source": [
    "loaded_vectorstore.vectorstore.index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producto: Anillo de diamantes. Color: blanco. Precio: 200.0. Descripción: Anillo con diamantes.. {'id': 2, 'color': 'blanco', 'price': 200.0}\n",
      "Producto: Anillo de diamantes. Color: blanco. Precio: 200.0. Descripción: Anillo con diamantes.. {'id': 4, 'color': 'blanco', 'price': 200.0}\n",
      "Producto: Collar de perlas. Color: blanco. Precio: 50.0. Descripción: Un elegante collar de perlas.. {'id': 3, 'color': 'blanco', 'price': 50.0}\n"
     ]
    }
   ],
   "source": [
    "query = \"Productos\"\n",
    "results = loaded_vectorstore.similarity_search(query, k=20, filter={\"color\": \"blanco\"})\n",
    "\n",
    "for result in results:\n",
    "    print(result.page_content, result.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"quiero productos de color blanco\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "# # Filtrar por metadatos si necesitas obtener productos de un color específico\n",
    "# filtered_results = [result for result in results if \"blanco\" in result.metadata.get(\"color\", \"\")]\n",
    "# print(filtered_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'color': 'blanco', 'id': 2, 'price': 200.0}, page_content='Anillo de diamantes'),\n",
       " Document(metadata={'color': 'blanco', 'id': 3, 'price': 50.0}, page_content='Collar de perlas'),\n",
       " Document(metadata={'color': 'rosa', 'id': 1, 'price': 50.0}, page_content='Collar de perlas')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'color': 'blanco', 'id': 2, 'price': 200.0}, page_content='Anillo de diamantes'),\n",
       " Document(metadata={'color': 'blanco', 'id': 2, 'price': 200.0}, page_content='Anillo de diamantes'),\n",
       " Document(metadata={'color': 'rosa', 'id': 1, 'price': 50.0}, page_content='Collar de perlas')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\", search_kwargs={\"k\": 3, \"fetch_k\": 5}\n",
    ")\n",
    "retriever.invoke(\"quiero productos de color blanco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'embeddings', 'documents', 'uris', 'data', 'metadatas', 'included'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_documents = vectorstore._collection.get()\n",
    "total_documents.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_documents[\"documents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    deployment_name=os.getenv(\"OPENAI_DEPLOIMENT_MODEL\"),\n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I said \"J'adore la programmation,\" which translates to \"I love programming\" in English.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "ai_msg = chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Translate from English to French: I love programming.\"\n",
    "            ),\n",
    "            AIMessage(content=\"J'adore la programmation.\"),\n",
    "            HumanMessage(content=\"What did you just say?\"),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import trim_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define trimmer\n",
    "# count each message as 1 \"token\" (token_counter=len) and keep only the last two messages\n",
    "trimmer = trim_messages(strategy=\"last\", max_tokens=2, token_counter=len)\n",
    "\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant. \"\n",
    "        \"Answer all questions to the best of your ability.\"\n",
    "    )\n",
    "    messages = [SystemMessage(content=system_prompt)] + trimmed_messages\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the node and edge\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(START, \"model\")\n",
    "\n",
    "# Add simple in-memory checkpointer\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='bb2071ba-1d01-4af6-a04c-c251054d02c0'),\n",
       "  AIMessage(content=\"J'aime programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-5d786197-1eaa-4d83-9b32-be5ace7954fe-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Translate to French: I love programming.\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"1\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, RemoveMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant. \"\n",
    "        \"Answer all questions to the best of your ability. \"\n",
    "        \"The provided chat history includes a summary of the earlier conversation.\"\n",
    "    )\n",
    "    system_message = SystemMessage(content=system_prompt)\n",
    "    message_history = state[\"messages\"][:-1]  # exclude the most recent user input\n",
    "    # Summarize the messages if the chat history reaches a certain size\n",
    "    if len(message_history) >= 4:\n",
    "        last_human_message = state[\"messages\"][-1]\n",
    "        # Invoke the model to generate conversation summary\n",
    "        summary_prompt = (\n",
    "            \"Distill the above chat messages into a single summary message. \"\n",
    "            \"Include as many specific details as you can.\"\n",
    "        )\n",
    "        summary_message = model.invoke(\n",
    "            message_history + [HumanMessage(content=summary_prompt)]\n",
    "        )\n",
    "\n",
    "        # Delete messages that we no longer want to show up\n",
    "        delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"]]\n",
    "        # Re-add user message\n",
    "        human_message = HumanMessage(content=last_human_message.content)\n",
    "        # Call the model with summary & response\n",
    "        response = model.invoke([system_message, summary_message, human_message])\n",
    "        message_updates = [summary_message, human_message, response] + delete_messages\n",
    "    else:\n",
    "        message_updates = model.invoke([system_message] + state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": message_updates}\n",
    "\n",
    "\n",
    "# Define the node and edge\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(START, \"model\")\n",
    "\n",
    "# Add simple in-memory checkpointer\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: [AIMessage(content='The user repeatedly asked, \"What did I just ask you?\" three times in succession, prompting a response that acknowledged their inquiry.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 76, 'total_tokens': 102, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-2ee8d9b7-5d2b-42ea-83a6-a0b0529acc92-0', usage_metadata={'input_tokens': 76, 'output_tokens': 26, 'total_tokens': 102, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What did I just ask you?', additional_kwargs={}, response_metadata={}, id='5fc27f0d-e7a0-4253-8980-08c5a46b3623'), AIMessage(content='You just asked, \"What did I just ask you?\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 76, 'total_tokens': 88, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-b84dd394-34c8-48c9-94da-410ec2142544-0', usage_metadata={'input_tokens': 76, 'output_tokens': 12, 'total_tokens': 88, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What did I just ask you?', additional_kwargs={}, response_metadata={}, id='ffb4da33-879d-447f-ab22-0494f18c8767'), AIMessage(content='You just asked, \"What did I just ask you?\" again.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 103, 'total_tokens': 117, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-cf41e65d-add6-4d1d-9702-238cc1c06321-0', usage_metadata={'input_tokens': 103, 'output_tokens': 14, 'total_tokens': 117, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='Holaa', additional_kwargs={}, response_metadata={}, id='f07bef2e-55f3-4217-b7a6-c495ec53d729')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'¡Hola! ¿Cómo puedo ayudarte hoy?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Holaa\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"1\"}},\n",
    ")\n",
    "\n",
    "a[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 1: No has hecho ninguna pregunta anterior en este chat.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Crear el objeto de embeddings\n",
    "embedding_function = GPT4AllEmbeddings()\n",
    "\n",
    "# Cargar la base de datos existente\n",
    "persist_directory = \"./data/chroma_langchain_db\"\n",
    "vectorstore = Chroma(collection_name=\"products\", \n",
    "                     embedding_function=embedding_function, \n",
    "                     persist_directory=persist_directory)\n",
    "\n",
    "# Configurar el retriever\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5, \"fetch_k\": len(vectorstore._collection.get()[\"documents\"])})\n",
    "\n",
    "# Crear la memoria\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, input_key=\"question\")\n",
    "\n",
    "# Crear el prompt\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"chat_history\", \"question\"],\n",
    "    template=(\n",
    "        \"You are a helpful assistant. Use the context below to answer the question.\"\n",
    "        \"\\n\\nChat History:\\n{chat_history}\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAnswer:\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Crear la cadena RetrievalQA con memoria\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=model,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt, \"memory\": memory}\n",
    ")\n",
    "\n",
    "# Usar la cadena con memoria\n",
    "query_1 = \"cual ha sido mi última pregunta?\"\n",
    "result_1 = qa_chain({\"query\": query_1})\n",
    "print(\"Answer 1:\", result_1[\"result\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 11, updating n_results = 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Simulated response based on context: You are a helpful assistant. Use the following context to answer the question.\\n\\nContext:\\nAnillo de diamantes\\n\\nAnillo de diamantes\\n\\nAnillo de diamantes\\n\\nAnillo de diamantes\\n\\nCollar de perlas\\n\\nQuestion: qué te pregunte antes?' additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Crear el objeto de embeddings (esto depende de tu configuración exacta)\n",
    "# Asegúrate de reemplazar GPT4AllEmbeddings con la correcta si es necesario\n",
    "embedding_function = GPT4AllEmbeddings()\n",
    "\n",
    "# Cargar la base de datos existente\n",
    "persist_directory = \"./data/chroma_langchain_db\"\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"products\",\n",
    "    embedding_function=embedding_function,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "# Configurar el retriever\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5})\n",
    "\n",
    "# Formatear la entrada para el modelo\n",
    "def format_input_to_model(query, retrieved_docs):\n",
    "    \"\"\"Formatear el input del modelo con el prompt inicial y los documentos recuperados.\"\"\"\n",
    "    system_prompt = \"You are a helpful assistant. Use the following context to answer the question.\"\n",
    "    context = \"\\n\\n\".join(retrieved_docs)\n",
    "    final_prompt = f\"{system_prompt}\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\"\n",
    "    return [SystemMessage(content=system_prompt), HumanMessage(content=final_prompt)]\n",
    "\n",
    "# Modelo simulado para generar una respuesta\n",
    "def dummy_model(messages):\n",
    "    \"\"\"Simular la respuesta del modelo.\"\"\"\n",
    "    system_message, human_message = messages\n",
    "    response_content = f\"Simulated response based on context: {human_message.content}\"\n",
    "    return AIMessage(content=response_content)\n",
    "\n",
    "# Pipeline manual\n",
    "def run_pipeline(query):\n",
    "    # Recuperar documentos\n",
    "    retrieved_docs = [doc.page_content for doc in retriever.get_relevant_documents(query)]\n",
    "    # Formatear entrada para el modelo\n",
    "    messages = format_input_to_model(query, retrieved_docs)\n",
    "    # Llamar al modelo\n",
    "    response = dummy_model(messages)\n",
    "    return response\n",
    "\n",
    "# Ejecutar el pipeline\n",
    "query = \"qué te pregunte antes?\"\n",
    "result = run_pipeline(query)\n",
    "\n",
    "# Imprimir respuesta final\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.chains import RunnableMap, RunnablePassthrough\n",
    "from langchain.chains.base import Runnable\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# 1. Definir el retriever personalizado\n",
    "class CustomRetriever(Runnable):\n",
    "    def __init__(self, vectorstore):\n",
    "        self.vectorstore = vectorstore\n",
    "\n",
    "    def invoke(self, query):\n",
    "        # Lógica de recuperación basada en la función proporcionada\n",
    "        all_doc = self.vectorstore._collection.get()[\"documents\"]\n",
    "        total_len_doc = len(all_doc)\n",
    "        \n",
    "        # Recuperar documentos usando el vectorstore\n",
    "        retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"mmr\", search_kwargs={\"k\": 10, \"fetch_k\": total_len_doc}\n",
    "        )\n",
    "        result = retriever.invoke(query)\n",
    "\n",
    "        # Procesar los resultados\n",
    "        output_ids = []\n",
    "        output_text = []\n",
    "        for doc in result:\n",
    "            if doc.metadata[\"id\"] not in output_ids:\n",
    "                output_ids.append(doc.metadata[\"id\"])\n",
    "                output_text.append(doc.page_content)\n",
    "\n",
    "        return output_text\n",
    "\n",
    "# 2. Formatear la entrada para el modelo\n",
    "def format_input_to_model(inputs):\n",
    "    \"\"\"Formatear el input del modelo con el prompt inicial y los documentos recuperados.\"\"\"\n",
    "    query = inputs[\"query\"]\n",
    "    retrieved_docs = inputs[\"retrieved_docs\"]\n",
    "\n",
    "    # Crear el prompt del sistema\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant. Use the following context to answer the question.\"\n",
    "    )\n",
    "    \n",
    "    # Crear contexto con los documentos recuperados\n",
    "    context = \"\\n\\n\".join(retrieved_docs)\n",
    "    \n",
    "    # Formatear el prompt final\n",
    "    final_prompt = f\"{system_prompt}\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\"\n",
    "\n",
    "    return [SystemMessage(content=system_prompt), HumanMessage(content=final_prompt)]\n",
    "\n",
    "# 3. Supongamos que el modelo es un LLM ya cargado, como OpenAI, o un modelo previamente inicializado\n",
    "class DummyModel(Runnable):\n",
    "    def invoke(self, inputs):\n",
    "        # Aquí simulamos la respuesta del modelo.\n",
    "        query = inputs['query']\n",
    "        context = inputs['context']\n",
    "        return AIMessage(content=f\"Answer based on context: {context} \\n\\nAnswer: We recommend considering the following aspects...\")\n",
    "\n",
    "# 4. Crear el flujo del pipeline\n",
    "vectorstore = ...  # Tu instancia de vectorstore aquí\n",
    "retriever = CustomRetriever(vectorstore)  # Instancia del retriever personalizado\n",
    "\n",
    "# 5. Definir las etapas del pipeline\n",
    "pipeline = RunnableMap({\n",
    "    \"retrieved_docs\": retriever,  # Etapa de recuperación de documentos\n",
    "    \"query\": RunnablePassthrough()  # Pasar la query directamente como entrada\n",
    "}) | RunnableMap({\n",
    "    \"messages\": format_input_to_model  # Formatear entrada para el modelo\n",
    "}) | DummyModel()  # Llamada al modelo (esto lo reemplazarías con tu modelo real)\n",
    "\n",
    "# 6. Ejecutar el pipeline\n",
    "query = \"What are the benefits of pink products?\"\n",
    "result = pipeline.invoke({\"query\": query})\n",
    "\n",
    "# 7. Imprimir respuesta final\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_manager import RAGManager\n",
    "\n",
    "rag_mngr = RAGManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_len_doc:4\n",
      "{\"id\": \"producto\", \"color\": \"color\", \"precio\": \"precio\"}\n",
      "<class 'str'>\n",
      "{'id': 'producto', 'color': 'color', 'precio': 'precio'}\n",
      "<class 'dict'>\n",
      "filtros: {'id': 'producto', 'color': 'color', 'precio': 'precio'}\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_mngr.retrieve_data(\"quiero los productos para el cuello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_len_doc:4\n",
      "{}\n",
      "<class 'str'>\n",
      "{}\n",
      "<class 'dict'>\n",
      "filtros: {}\n",
      "[Document(id='54c4b021-1f68-486e-88dd-5a35c61d9ea2', metadata={'id': 2, 'color': 'blanco', 'price': 200.0}, page_content='Producto: Anillo de diamantes. Color: blanco. Precio: 200.0. Descripción: Anillo con diamantes..'), Document(id='831df401-1741-4ce0-bb94-218995cc4d9a', metadata={'id': 4, 'color': 'blanco', 'price': 200.0}, page_content='Producto: Anillo de diamantes. Color: blanco. Precio: 200.0. Descripción: Anillo con diamantes..'), Document(id='12da9b5d-53d2-4218-8be9-a9c87e5c2d4e', metadata={'id': 3, 'color': 'blanco', 'price': 50.0}, page_content='Producto: Collar de perlas. Color: blanco. Precio: 50.0. Descripción: Un elegante collar de perlas..'), Document(id='aa77003b-fe15-43bd-abbb-a89d06e9019e', metadata={'id': 1, 'color': 'rosa', 'price': 50.0}, page_content='Producto: Collar de perlas. Color: rosa. Precio: 50.0. Descripción: Un elegante collar de perlas..')]\n",
      "output_text: ['Producto: Anillo de diamantes. Color: blanco. Precio: 200.0. Descripción: Anillo con diamantes..', 'Producto: Anillo de diamantes. Color: blanco. Precio: 200.0. Descripción: Anillo con diamantes..', 'Producto: Collar de perlas. Color: blanco. Precio: 50.0. Descripción: Un elegante collar de perlas..', 'Producto: Collar de perlas. Color: rosa. Precio: 50.0. Descripción: Un elegante collar de perlas..']\n",
      "output_ids: [2, 4, 3, 1]\n",
      "Base de datos a usar para la respuesta: ['Producto: Anillo de diamantes. Color: blanco. Precio: 200.0. Descripción: Anillo con diamantes..', 'Producto: Anillo de diamantes. Color: blanco. Precio: 200.0. Descripción: Anillo con diamantes..', 'Producto: Collar de perlas. Color: blanco. Precio: 50.0. Descripción: Un elegante collar de perlas..', 'Producto: Collar de perlas. Color: rosa. Precio: 50.0. Descripción: Un elegante collar de perlas..'], Pregunta: qué te he preguntado anteriormente\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Anteriormente me preguntaste por productos para el cuello y de color blanco. Te proporcioné información sobre el \"Collar de perlas\" que es blanco y adecuado para el cuello. Si necesitas más ayuda o información, no dudes en preguntar.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_mngr.chatbot(\"qué te he preguntado anteriormente\", 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
